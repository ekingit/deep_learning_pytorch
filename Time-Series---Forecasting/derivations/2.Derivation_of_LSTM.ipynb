{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b76dc86-4b57-4d80-82cd-b396ee42a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6433e-30c4-4316-9a9a-a776c17c2462",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) specifically designed to capture long-term dependencies in sequential data. Unlike standard RNNs, which struggle with vanishing or exploding gradients when learning long-range dependencies, LSTMs use a set of gates—input, forget, and output gates—to control the flow of information. These gates regulate what information is retained, forgotten, or passed to the next step, allowing LSTMs to effectively remember information over long sequences. Of course, this comes with a price. These gates are dynamical, learnt during the training from data. LSTMs have 3 times more parameters than RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69235120-8e1b-4bf1-b02b-5d743c766744",
   "metadata": {},
   "source": [
    "![something](https://d2l.ai/_images/lstm-0.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ece823-51be-4ce1-b6a0-c671e9740182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1])\n",
      "torch.Size([12, 3])\n"
     ]
    }
   ],
   "source": [
    "model = nn.LSTM(1,3,bias=False)\n",
    "A = list(model.parameters())\n",
    "for i in A:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb15c4f2-3012-4b7d-84bf-ba809fd2bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3]), torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(100,1)\n",
    "H = torch.zeros(1,3)\n",
    "C = torch.zeros(1,3)\n",
    "alpha, (beta,gamma) = model(X, (H,C))\n",
    "alpha.shape, beta.shape, gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42feee34-53e6-438e-b1e3-71e8360875fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_scratch(X, H, C, W_1, W_2,hid=3):\n",
    "    W_1 = [W_1[hid*i:hid*(i+1)].transpose(0,1) for i in range(4)]\n",
    "    W_2 = [W_2[hid*i:hid*(i+1)].transpose(0,1) for i in range(4)]\n",
    "        \n",
    "    out_scratch = []\n",
    "    for i in range(0,len(X)):\n",
    "        I = torch.sigmoid(torch.matmul(X[i],W_1[0])+torch.matmul(H,W_2[0]))\n",
    "        F = torch.sigmoid(torch.matmul(X[i],W_1[1])+torch.matmul(H,W_2[1]))\n",
    "        C_tilde = torch.tanh(torch.matmul(X[i],W_1[2])+torch.matmul(H,W_2[2]))\n",
    "        O = torch.sigmoid(torch.matmul(X[i],W_1[3])+torch.matmul(H,W_2[3]))\n",
    "        C = F*C+I*C_tilde\n",
    "        H = O*torch.tanh(C)\n",
    "        out_scratch.append(H)\n",
    "    out_scratch = torch.stack(out_scratch).reshape(len(X),hid)\n",
    "    return out_scratch, (H,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004b780d-8f38-4a7a-82e4-167ac2ba205d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3]), torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, (b,c) = LSTM_scratch(X,H,C,A[0],A[1],3)\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0b4f65-a786-43ad-bdcd-d5ba6159a074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(a,alpha), torch.allclose(b, beta), torch.allclose(c,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ff9d1-47f0-4239-bd70-7736c0f14988",
   "metadata": {},
   "source": [
    "**Remark:**\n",
    "$H$ is $(1,hid)$-dimensional matrix and $W_2$ is $(hid\\times hid)$ dimensional square matrix. Contrary to the [documantation of LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), we cannot multiply $W_2$ by H directly. The multiplication should be either $W_2\\cdot H^T$ or $H\\cdot W_2^T$. [A pull-request](https://github.com/pytorch/pytorch/pull/138191) has been submitted to address this issue in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e89d2-ffb0-4506-9083-984303d73a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
