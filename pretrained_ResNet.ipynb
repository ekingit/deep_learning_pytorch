{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekingit/CNN-for-CIFAR10/blob/main/pretrained_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7zmnEbrpTO5N"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "dP0rGec0UFdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mean and std. dev. for the pretrained model\n",
        "mu = torch.tensor([0.485, 0.456, 0.406]) #  (3)\n",
        "sigma = torch.tensor([0.229, 0.224, 0.225]) # (3)\n",
        "\n",
        "v2_train = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize(mu, sigma),])\n",
        "v2_test = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize(mu, sigma),])"
      ],
      "metadata": {
        "id": "tSBUHtg8UxHT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = datasets.CIFAR10('./data', train=True, download=True, transform=v2_train)\n",
        "ds_test = datasets.CIFAR10('./data', train=False, download=True, transform=v2_test)\n",
        "dl_train = DataLoader(ds_train, batch_size=128,shuffle=True) #50.000 item = 128*390.625 = 128*390+80\n",
        "dl_test = DataLoader(ds_test, batch_size=128,shuffle=True) #10.000 item = 128*78.125 = 128*78+16"
      ],
      "metadata": {
        "id": "zlMhWa_sUus9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5c1f59-c172-4bb0-abe3-5fde8147bb29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:11<00:00, 14.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "-NXL4yP9V8g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ResNet18 trained on ImageNet\n",
        "pretrained_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "total_parameters = sum(p.numel() for p in pretrained_model.parameters())\n",
        "print(f\"Total Parameters: {total_parameters:,}\") #12 million parameters"
      ],
      "metadata": {
        "id": "y-mCT1ZeJ3ns",
        "outputId": "65c6f75c-5a78-4273-ddc6-007508b40ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 11,689,512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's take a look at the layers of the model\n",
        "for name, layer in pretrained_model.named_children():\n",
        "    print(name, layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaC37jKVOiDN",
        "outputId": "c6c293b1-ac24-4be1-fc06-a7aa467fa609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "relu ReLU(inplace=True)\n",
            "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "layer1 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer2 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer3 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer4 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "fc Linear(in_features=512, out_features=1000, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#While all layers in this model extract features from images, the final layer classifies the image into one of 1,000 labels.\n",
        "pretrained_model.fc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aHnMmR1PQLe",
        "outputId": "350cdcd5-3093-4f1c-98c9-a936a17f7774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=1000, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We modify the last layer to classify into 10 classes, as required for CIFAR-10.\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
        "nn.init.xavier_uniform_(pretrained_model.fc.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rdp0xRcPMDu",
        "outputId": "966c2b58-c875-4348-a71f-23646685ba64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0388, -0.1046,  0.0758,  ...,  0.0668, -0.0284,  0.0834],\n",
              "        [ 0.1063,  0.0632, -0.0109,  ..., -0.0014,  0.0328,  0.0672],\n",
              "        [ 0.0601, -0.0880, -0.0404,  ..., -0.0346,  0.0763,  0.0432],\n",
              "        ...,\n",
              "        [-0.0761,  0.0398,  0.0946,  ...,  0.0214,  0.0443, -0.0795],\n",
              "        [-0.0573,  0.0936,  0.0144,  ..., -0.0964,  0.0273, -0.0672],\n",
              "        [ 0.0799, -0.0011, -0.0401,  ..., -0.0927, -0.0899, -0.0725]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dl, optimizer, epoch, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.train() #from nn.Module\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    for data, target in dl:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data) #(batch_size,3,32,32) --> (10)\n",
        "        batch_loss = loss(output, target) #(10x10) --> 1\n",
        "        batch_loss.backward() #calculates gradients\n",
        "        optimizer.step() #updates weights and kernels\n",
        "        train_loss += batch_loss.detach().item()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item() # get the correct predictions\n",
        "\n",
        "    correct = 100. * correct / len(dl.dataset)\n",
        "    train_loss /= len(dl)\n",
        "    trainloss.append(train_loss)\n",
        "    trainacc.append(correct)\n",
        "    print(f'Epoch: {epoch}, Avarage train loss: {train_loss:.2f}, Accuracy: {correct:.1f}%')"
      ],
      "metadata": {
        "id": "bJX7IIe7QQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dl_test, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # no need to track gradients. Saves memory.\n",
        "        for data, target in dl_test:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            batch_loss = loss(output,target)\n",
        "            test_loss += batch_loss.detach().item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # get the correct predictions\n",
        "\n",
        "    test_loss /= len(dl_test)\n",
        "    correct = 100. * correct / len(dl_test.dataset)\n",
        "    testloss.append(test_loss)\n",
        "    testacc.append(correct)\n",
        "    print(f'Avarage test loss: {test_loss:.2f}, Accuracy: {correct:.1f}%')"
      ],
      "metadata": {
        "id": "jdjscJqYQYbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "fvGX5ew7Q8uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All model parameters, except those of the final classification layer, are initialized using a pretrained model. During training, we aim to fine-tune these parameters while also training the final layer's parameters. To achieve this, we assign a larger learning rate to the last layer—specifically, 10 times the learning rate of the other layers."
      ],
      "metadata": {
        "id": "c6ypSe8RyzL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 5e-5\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-5\n",
        "epochs = 50\n",
        "\n",
        "torch.manual_seed(4321)\n",
        "\n",
        "device = 'cuda'\n",
        "if device == 'cuda': torch.backends.cudnn.benchmark = True # additional speed up\n",
        "\n",
        "#\n",
        "params = [param for name, param in pretrained_model.named_parameters() if name not in [\"fc.weight\", \"fc.bias\"]] # all params except the last layer\n",
        "optimizer = torch.optim.SGD([{'params': params}, {'params': pretrained_model.fc.parameters(), 'lr': 10 * lr}], lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# run()\n",
        "a = time.time()\n",
        "trainloss = []\n",
        "testloss = []\n",
        "trainacc = []\n",
        "testacc = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    train(pretrained_model, dl_train, optimizer, epoch+1, device=device)\n",
        "    test(pretrained_model, dl_test, device=device)\n",
        "    scheduler.step()\n",
        "\n",
        "b = time.time()\n",
        "print(f'Training took {round(b - a, 0)} seconds.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5WL6O5kQgKG",
        "outputId": "40177d81-1858-483d-8315-004dd59c54be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Avarage train loss: 1.96, Accuracy: 35.9%\n",
            "Avarage test loss: 1.56, Accuracy: 45.9%\n",
            "Epoch: 2, Avarage train loss: 1.40, Accuracy: 51.0%\n",
            "Avarage test loss: 1.34, Accuracy: 53.3%\n",
            "Epoch: 3, Avarage train loss: 1.23, Accuracy: 56.9%\n",
            "Avarage test loss: 1.24, Accuracy: 57.4%\n",
            "Epoch: 4, Avarage train loss: 1.13, Accuracy: 60.6%\n",
            "Avarage test loss: 1.16, Accuracy: 59.5%\n",
            "Epoch: 5, Avarage train loss: 1.05, Accuracy: 63.1%\n",
            "Avarage test loss: 1.12, Accuracy: 61.4%\n",
            "Epoch: 6, Avarage train loss: 0.98, Accuracy: 65.4%\n",
            "Avarage test loss: 1.07, Accuracy: 63.0%\n",
            "Epoch: 7, Avarage train loss: 0.93, Accuracy: 67.3%\n",
            "Avarage test loss: 1.03, Accuracy: 63.9%\n",
            "Epoch: 8, Avarage train loss: 0.88, Accuracy: 68.9%\n",
            "Avarage test loss: 1.01, Accuracy: 65.2%\n",
            "Epoch: 9, Avarage train loss: 0.84, Accuracy: 70.2%\n",
            "Avarage test loss: 0.99, Accuracy: 65.8%\n",
            "Epoch: 10, Avarage train loss: 0.81, Accuracy: 71.6%\n",
            "Avarage test loss: 0.97, Accuracy: 66.4%\n",
            "Epoch: 11, Avarage train loss: 0.78, Accuracy: 72.7%\n",
            "Avarage test loss: 0.96, Accuracy: 67.0%\n",
            "Epoch: 12, Avarage train loss: 0.74, Accuracy: 73.9%\n",
            "Avarage test loss: 0.94, Accuracy: 67.6%\n",
            "Epoch: 13, Avarage train loss: 0.72, Accuracy: 74.9%\n",
            "Avarage test loss: 0.94, Accuracy: 68.0%\n",
            "Epoch: 14, Avarage train loss: 0.69, Accuracy: 75.8%\n",
            "Avarage test loss: 0.92, Accuracy: 68.4%\n",
            "Epoch: 15, Avarage train loss: 0.66, Accuracy: 76.8%\n",
            "Avarage test loss: 0.92, Accuracy: 69.0%\n",
            "Epoch: 16, Avarage train loss: 0.64, Accuracy: 77.5%\n",
            "Avarage test loss: 0.91, Accuracy: 69.2%\n",
            "Epoch: 17, Avarage train loss: 0.62, Accuracy: 78.3%\n",
            "Avarage test loss: 0.90, Accuracy: 69.5%\n",
            "Epoch: 18, Avarage train loss: 0.60, Accuracy: 78.9%\n",
            "Avarage test loss: 0.90, Accuracy: 69.9%\n",
            "Epoch: 19, Avarage train loss: 0.58, Accuracy: 79.6%\n",
            "Avarage test loss: 0.89, Accuracy: 70.1%\n",
            "Epoch: 20, Avarage train loss: 0.56, Accuracy: 80.5%\n",
            "Avarage test loss: 0.89, Accuracy: 70.1%\n",
            "Epoch: 21, Avarage train loss: 0.54, Accuracy: 81.1%\n",
            "Avarage test loss: 0.89, Accuracy: 70.7%\n",
            "Epoch: 22, Avarage train loss: 0.52, Accuracy: 81.6%\n",
            "Avarage test loss: 0.88, Accuracy: 70.9%\n",
            "Epoch: 23, Avarage train loss: 0.50, Accuracy: 82.3%\n",
            "Avarage test loss: 0.88, Accuracy: 70.9%\n",
            "Epoch: 24, Avarage train loss: 0.48, Accuracy: 83.3%\n",
            "Avarage test loss: 0.89, Accuracy: 71.1%\n",
            "Epoch: 25, Avarage train loss: 0.47, Accuracy: 83.6%\n",
            "Avarage test loss: 0.89, Accuracy: 71.0%\n",
            "Epoch: 26, Avarage train loss: 0.45, Accuracy: 84.4%\n",
            "Avarage test loss: 0.88, Accuracy: 71.4%\n",
            "Epoch: 27, Avarage train loss: 0.44, Accuracy: 84.9%\n",
            "Avarage test loss: 0.90, Accuracy: 71.5%\n",
            "Epoch: 28, Avarage train loss: 0.42, Accuracy: 85.5%\n",
            "Avarage test loss: 0.89, Accuracy: 71.6%\n",
            "Epoch: 29, Avarage train loss: 0.41, Accuracy: 85.9%\n",
            "Avarage test loss: 0.89, Accuracy: 71.7%\n",
            "Epoch: 30, Avarage train loss: 0.39, Accuracy: 86.5%\n",
            "Avarage test loss: 0.89, Accuracy: 71.6%\n",
            "Epoch: 31, Avarage train loss: 0.38, Accuracy: 86.9%\n",
            "Avarage test loss: 0.90, Accuracy: 71.9%\n",
            "Epoch: 32, Avarage train loss: 0.36, Accuracy: 87.5%\n",
            "Avarage test loss: 0.90, Accuracy: 71.7%\n",
            "Epoch: 33, Avarage train loss: 0.35, Accuracy: 88.0%\n",
            "Avarage test loss: 0.91, Accuracy: 71.7%\n",
            "Epoch: 34, Avarage train loss: 0.35, Accuracy: 88.2%\n",
            "Avarage test loss: 0.91, Accuracy: 71.8%\n",
            "Epoch: 35, Avarage train loss: 0.32, Accuracy: 88.9%\n",
            "Avarage test loss: 0.92, Accuracy: 72.1%\n",
            "Epoch: 36, Avarage train loss: 0.31, Accuracy: 89.3%\n",
            "Avarage test loss: 0.92, Accuracy: 71.7%\n",
            "Epoch: 37, Avarage train loss: 0.30, Accuracy: 90.1%\n",
            "Avarage test loss: 0.93, Accuracy: 71.9%\n",
            "Epoch: 38, Avarage train loss: 0.29, Accuracy: 90.1%\n",
            "Avarage test loss: 0.94, Accuracy: 72.1%\n",
            "Epoch: 39, Avarage train loss: 0.28, Accuracy: 90.5%\n",
            "Avarage test loss: 0.93, Accuracy: 72.1%\n",
            "Epoch: 40, Avarage train loss: 0.27, Accuracy: 91.0%\n",
            "Avarage test loss: 0.94, Accuracy: 72.1%\n",
            "Epoch: 41, Avarage train loss: 0.26, Accuracy: 91.3%\n",
            "Avarage test loss: 0.95, Accuracy: 72.2%\n",
            "Epoch: 42, Avarage train loss: 0.25, Accuracy: 91.7%\n",
            "Avarage test loss: 0.95, Accuracy: 72.3%\n",
            "Epoch: 43, Avarage train loss: 0.24, Accuracy: 92.1%\n",
            "Avarage test loss: 0.95, Accuracy: 72.2%\n",
            "Epoch: 44, Avarage train loss: 0.24, Accuracy: 92.2%\n",
            "Avarage test loss: 0.98, Accuracy: 72.3%\n",
            "Epoch: 45, Avarage train loss: 0.22, Accuracy: 92.8%\n",
            "Avarage test loss: 0.97, Accuracy: 72.3%\n",
            "Epoch: 46, Avarage train loss: 0.22, Accuracy: 92.9%\n",
            "Avarage test loss: 0.98, Accuracy: 72.7%\n",
            "Epoch: 47, Avarage train loss: 0.21, Accuracy: 93.3%\n",
            "Avarage test loss: 0.99, Accuracy: 72.6%\n",
            "Epoch: 48, Avarage train loss: 0.20, Accuracy: 93.7%\n",
            "Avarage test loss: 1.02, Accuracy: 72.4%\n",
            "Epoch: 49, Avarage train loss: 0.19, Accuracy: 94.0%\n",
            "Avarage test loss: 1.00, Accuracy: 72.6%\n",
            "Epoch: 50, Avarage train loss: 0.18, Accuracy: 94.4%\n",
            "Avarage test loss: 1.02, Accuracy: 72.5%\n",
            "Training took 1589.0 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the effectiveness of pretraining, let's also train the model with random initialization for comparison."
      ],
      "metadata": {
        "id": "hQcN2zrW184I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet18(pretrained=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CNQf3r-0pNk",
        "outputId": "de20cc7a-e197-4f8d-c16b-c08e67edfec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "nn.init.xavier_uniform_(model.fc.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMUXwlHB1GHI",
        "outputId": "b52a4616-7c4d-4b66-cd7d-5c166ee76472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0790, -0.0576, -0.0450,  ..., -0.0095,  0.0788,  0.0545],\n",
              "        [ 0.0374,  0.1041, -0.0254,  ...,  0.0206,  0.0418, -0.0794],\n",
              "        [-0.0456,  0.0770,  0.0245,  ..., -0.0440,  0.1064,  0.0973],\n",
              "        ...,\n",
              "        [ 0.0055,  0.0475, -0.0400,  ..., -0.0041, -0.0834,  0.0660],\n",
              "        [ 0.0418,  0.0817,  0.0433,  ..., -0.0074, -0.0647,  0.0371],\n",
              "        [ 0.0039,  0.0132,  0.0398,  ...,  0.0661,  0.0467, -0.1025]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 5e-4\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-5\n",
        "epochs = 20\n",
        "\n",
        "torch.manual_seed(4321) #fixes initial weights and kernels to test different models/hyperparameters\n",
        "\n",
        "device = 'cuda'\n",
        "if device == 'cuda': torch.backends.cudnn.benchmark = True # additional speed up\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-5)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# run()\n",
        "a = time.time()\n",
        "trainloss = []\n",
        "testloss = []\n",
        "trainacc = []\n",
        "testacc = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    train(model, dl_train, optimizer, epoch+1, device=device)\n",
        "    test(model, dl_test, device=device)\n",
        "    scheduler.step()\n",
        "\n",
        "b = time.time()\n",
        "print(f'Training took {round(b - a, 0)} seconds.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dAaBE78RGyW",
        "outputId": "aef06726-0419-4a28-81df-a75a7780e463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Avarage train loss: 1.90, Accuracy: 35.0%\n",
            "Avarage test loss: 1.59, Accuracy: 44.0%\n",
            "Epoch: 2, Avarage train loss: 1.43, Accuracy: 48.9%\n",
            "Avarage test loss: 1.43, Accuracy: 48.8%\n",
            "Epoch: 3, Avarage train loss: 1.23, Accuracy: 56.2%\n",
            "Avarage test loss: 1.36, Accuracy: 51.9%\n",
            "Epoch: 4, Avarage train loss: 1.07, Accuracy: 62.1%\n",
            "Avarage test loss: 1.34, Accuracy: 52.9%\n",
            "Epoch: 5, Avarage train loss: 0.93, Accuracy: 67.3%\n",
            "Avarage test loss: 1.35, Accuracy: 53.9%\n",
            "Epoch: 6, Avarage train loss: 0.80, Accuracy: 72.2%\n",
            "Avarage test loss: 1.35, Accuracy: 54.6%\n",
            "Epoch: 7, Avarage train loss: 0.67, Accuracy: 77.3%\n",
            "Avarage test loss: 1.38, Accuracy: 55.1%\n",
            "Epoch: 8, Avarage train loss: 0.55, Accuracy: 81.9%\n",
            "Avarage test loss: 1.44, Accuracy: 54.3%\n",
            "Epoch: 9, Avarage train loss: 0.43, Accuracy: 86.4%\n",
            "Avarage test loss: 1.50, Accuracy: 54.4%\n",
            "Epoch: 10, Avarage train loss: 0.33, Accuracy: 90.0%\n",
            "Avarage test loss: 1.59, Accuracy: 54.5%\n",
            "Epoch: 11, Avarage train loss: 0.25, Accuracy: 92.9%\n",
            "Avarage test loss: 1.67, Accuracy: 54.5%\n",
            "Epoch: 12, Avarage train loss: 0.18, Accuracy: 95.3%\n",
            "Avarage test loss: 1.76, Accuracy: 55.3%\n",
            "Epoch: 13, Avarage train loss: 0.13, Accuracy: 96.8%\n",
            "Avarage test loss: 1.85, Accuracy: 55.0%\n",
            "Epoch: 14, Avarage train loss: 0.09, Accuracy: 97.9%\n",
            "Avarage test loss: 1.91, Accuracy: 55.0%\n",
            "Epoch: 15, Avarage train loss: 0.08, Accuracy: 98.3%\n",
            "Avarage test loss: 1.99, Accuracy: 55.1%\n",
            "Epoch: 16, Avarage train loss: 0.06, Accuracy: 98.7%\n",
            "Avarage test loss: 2.05, Accuracy: 55.4%\n",
            "Epoch: 17, Avarage train loss: 0.05, Accuracy: 99.1%\n",
            "Avarage test loss: 2.09, Accuracy: 55.9%\n",
            "Epoch: 18, Avarage train loss: 0.04, Accuracy: 99.3%\n",
            "Avarage test loss: 2.15, Accuracy: 55.5%\n",
            "Epoch: 19, Avarage train loss: 0.03, Accuracy: 99.4%\n",
            "Avarage test loss: 2.16, Accuracy: 55.6%\n",
            "Epoch: 20, Avarage train loss: 0.03, Accuracy: 99.5%\n",
            "Avarage test loss: 2.16, Accuracy: 55.6%\n",
            "Training took 639.0 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, model overfits very rapidly."
      ],
      "metadata": {
        "id": "Li9qzypQ2H-X"
      }
    }
  ]
}